% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Chapter: Experiment 2
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\chapter{Experiment 2: Smarter values generation}
\label{cpt:experiment2}
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Smarter values generation
%\section{Smarter values generation}
%\label{sec:ch4_smarter_values_generation}
\pinfo{Context - result of experiment 1, what aim is now}
Some bugs were found by using random input values in the first experiment.
However, the implicative properties were not effectively checked in terms of
triggering the if-clause when using random input values. This is what we aim to
improve in this experiment, expecting to detect more bugs.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Method
\section{Method}
\label{sct:experiment2_method}
\pinfo{2 categories separation}
We can separate the properties that we have in 2 different categories: those
using implication ($\implies$) and those that do not. The defined properties are
being separated over two specifications according to which category these
belong. We name these specifications \textit{MoneyExpressions} and
\textit{MoneyConditionals}. For the \textit{MoneyConditionals} specification
(the implicative properties), another way of generating the random values is
more useful. The random input values are being optimized such that the condition
of the if-clause of these properties are being satisfied. For the other
specification (\textit{MoneyExpressions}), the earlier approach (random values
as input) can still be used. We do not need to change this functionality for
these cases since the properties can be used with random values.\\
\\
\pinfo{Adding preconditions}
In the \textit{MoneyConditionals} specification, the condition to trigger the
if-clauses will be added to the preconditions of each the event definition, such
that these can be used to generate the values matching this clause. The updated
event definition of the \textit{Symmetric} property is shown in
\autoref{lst:experiment2_updated_definition} for example. Where the
preconditions have been added to the event definition.
% Listing
\FloatBarrier
\begin{sourcecode}[!ht]
\begin{lstlisting}[language=Rebel]
event symmetric(x: Money, y:Money) {
    preconditions {
        x == y;
    }
    postconditions {
       new this.result == ( (x == y) ? y == x : False );
    }
}
\end{lstlisting}
\caption{The updated event definition of the \textit{Symmetric} property}
\label{lst:experiment2_updated_definition}
\end{sourcecode}
\FloatBarrier
% End listing
\pinfo{Generating checks for preconditions}
When generating the test suite, the events are being traversed. In case an
event with some preconditions is found, it generates a list of value tuples that
satisfy the condition to trigger the if-clause. Which is different compared to
the generated tests in \autoref{cpt:experiment1}. The size of the tuples depends
on the arity of the event from which the test is being generated.\\
\\
\pinfo{Diff1: custom generator}
The first difference is that it now uses our custom generator to determine the
input values, instead of the built-in Java random generator. A list of tuples,
containing values which satisfy the if-clause of the implication, is being
generated. Our custom generator is a simple proof of concept in order to check
if this will actually result in more failing tests. This custom generator
basically consists of multiple methods which are being called based on the event
name. In \autoref{lst:ch4_second_generating_values} this behaviour is shown for
the \textit{Symmetric} and \textit{Division1} event. The \textit{String}
parameter of these methods is a way how we can pattern match on the event name
in \textit{Rascal}. In case the event couldn't be handled, an exception is
thrown.
% Listing
\FloatBarrier
\begin{sourcecode}[!ht]
\begin{lstlisting}[language=Rascal]
private list[Expr] genTestValueForEvent("Symmetric") {
    Expr moneyValue = genRandomMoney();
    return [moneyValue, moneyValue];
}
private list[Expr] genTestValueForEvent("Division1") {
    real moneyAmountX = genRandomDouble();
    real intAmountY = genRandomInteger();
    real moneyAmountZ = moneyAmountX * intAmountY;
    str currency = genRandomCurrency();
    return [convertToMoney(currency, moneyAmountX), converToExpr(intAmountY), convertToMoney(currency, moneyAmountZ)];
}
private default list[Expr] genTestValueForEvent(str eventName) {
    throw "genTestValueForEvent not implemented for event <eventName>";
}
\end{lstlisting}
\caption{Values generation for \textit{Symmetric} and \textit{Division1}, including the fall-back case.}
\label{lst:ch4_second_generating_values}
\end{sourcecode}
\FloatBarrier
% End listing
\pinfo{Not very dynamic}
This means that the way how we determine these values is basically hard-coded,
requiring to have knowledge about the if-clause itself. Note that this doesn't
make this approach very dynamic, but the result will consist of a list of tuples
that satisfy the if-clause. These tuples will be used as input for the test case
that will be generated.\\
\\
\pinfo{Mutating values with random operation}
However, the values that are generated now are fixed when we use them directly
in a test case, which completely removes the randomness of the values when
running the tests. It would be better to keep the randomness, such that the
values are different on each run. To solve this problem, we mutate the values in
the list such that the values are sort of random again. The tuples still have to
satisfy the condition to trigger the if-clause, as this was the actual
intention. So the second difference compared to the first experiment, is that
for each tuple in the list, we will generate a random operation and use that
operation to mutate the values inside the tuple. To ensure that the tuple values
still satisfy the condition of the if-clause, each value in the tuple will be
mutated by the same operation. In
\autoref{lst:experiment2_second_resulting_test} an example of a generated test
case is shown.
% Listing
\FloatBarrier
\begin{sourcecode}[!ht]
\begin{lstlisting}[language=Scala]
"work with Antisymmetry" in {
      Seq((USD(1593.62), USD(1593.62)), (USD(2869.78), USD(2869.78)),
          (EUR(4676.80), EUR(4676.80)), (USD(1850.29), USD(1850.29)),
          // ... // More values in the list
          (USD(9501.16), USD(9501.16)), (- EUR(149.67), - EUR(149.67)),
          (- EUR(159.67), - EUR(159.67)), (EUR(8015.77), EUR(8015.77)))
      .foreach {
        data:  (Money, Money) => {
          val randomOperation = genRandomOperation(genRandomOperator("Money", true), generateRandomMoney(data._1.currency), generateRandomInteger(true), generateRandomInteger(false), generateRandomPercentage(true), generateRandomPercentage(false), Random.nextInt(10))

          checkAction(Symmetry(
              randomOperation(data._1),
              randomOperation(data._2)
              )
          )
        }
      }
    }
\end{lstlisting}
\caption{Resulting test case with semi-random values. Omitted some input tuples for readability.}
\label{lst:experiment2_second_resulting_test}
\end{sourcecode}
\FloatBarrier
% End listing
\pinfo{Small explanation about the new test case}
The list of values are generated by using our custom generator, the number of
tuples in the list can be defined when generating the test suit. A method
\code{genRandomOperation()} has been added to the template, which is used to
mutate the fixed values in the list. After all the \code{checkAction()} method
is being called to check the result of the test.\\
\\
\pinfo{Also: else now returns false}
Now that the input values for the implication events should always satisfy the
condition of the if-clause, we can also update the specification such that the
else-clause of the expression always returns \textit{False}. This can be seen in
\autoref{lst:experiment2_updated_definition}. This results in a failing case
again in case the precondition was not met. When this happens, it could indicate
that there's a problem with either our custom generator or in the generator.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Results
\section{Results}
\pinfo{Failing tests: Division}
Running the test framework with these changes results in 2 additional failing tests
compared to the first experiment (\autoref{cpt:experiment1}). An overview of the
failing properties and the used input values are shown in
\autoref{tbl:experiment2_overview_first_run}. The log of the test run reports
that the precondition was not met when using these input values, as shown in
\autoref{lst:experiment2_log_first_run}.
% Table
\FloatBarrier
\begin{table}[!ht]
\centering
\begin{tabular}{llll}
\hline
\textbf{Property name} & \textbf{x}    & \textbf{y} & \textbf{z} \\ \hline
Division1              & -16729.90 USD & 830        & -20.16     \\
Division2              & -44.68 USD    & 870        & -38870.47  \\ \hline
\end{tabular}
\caption{Failing tests overview along with its input values}
\label{tbl:experiment2_overview_first_run}
\end{table}
\FloatBarrier
% End table

% Listing
\FloatBarrier
\begin{sourcecode}[!ht]
\begin{lstlisting}[language=Log]
[info] MoneyConditionals
[info] - should work with Additive4params (7 seconds, 224 milliseconds)
[info] - should work with AntisymmetryLET (5 seconds, 493 milliseconds)
[info] - should work with Symmetric (5 seconds, 344 milliseconds)
[info] - should work with Division2 *** FAILED *** (23 milliseconds)
[info]   java.lang.AssertionError: assertion failed: expected CommandSuccess(Division2(-16729.90 USD,830,-20.16 USD)), found CommandFailed(NonEmptyList(PreConditionFailed(x == z*y)))
[info] - should work with Division1 *** FAILED *** (127 milliseconds)
[info]   java.lang.AssertionError: assertion failed: expected CommandSuccess(Division1(-44.68 USD,870,-38870.47 USD)), found CommandFailed(NonEmptyList(PreConditionFailed(x*y == z)))
// ...
\end{lstlisting}
\caption{Precondition failed error in \textit{Division1} and \textit{Division2}.}
\label{lst:experiment2_log_first_run}
\end{sourcecode}
\FloatBarrier
% End listing

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Analysis
\section{Analysis}
The values used in the test case should be correct since we generated these
values such that they satisfy the condition of the if-clause and thus they
should satisfy the preconditions. Note that the conditions of the if-clause were
added as preconditions in the \textit{MoneyConditionals} specification, which
causes the error. As the \textit{PreConditionFailed} error is thrown by the
system when the input values do not satisfy the preconditions.\\
\\
\pinfo{Describe precision error happening at first}
For \textit{Division1} it states that the condition \code{x*y == z} failed. The
values used for \textit{x}, \textit{y} and \textit{z} were \textit{-44.68 USD},
\textit{870} and \textit{-38870.47 USD} respectively. The result of
\textit{x * y} = \textit{-44.68 USD * 870} = \textit{-38871.60 USD}. This should
be equal to \textit{z}. In fact, the input of \textit{z} was slightly different,
\textit{-38870.47 USD}.\\
\\
Remember that the input values are being mutated by a random operation that we
have added to the test cases. This difference is caused by the precision error
when operating with the \textit{Money} type, which was found in
\autoref{cpt:experiment1}. The random operation that was done was causing this
behaviour. The same goes for the error with \textit{Division2}, where
\code{x == z*y} should hold. The values of \textit{x}, \textit{y} and \textit{z}
are \textit{-16729.90 USD}, \textit{830}, \textit{-20.16 USD} respectively. The
result of \textit{z * y} = \textit{-16732.80 USD}, which is not equal to
\textit{-16729.90} USD.\\
\\
\pinfo{Next (when fixed precision), division problem}
The first experiment already described the precision problem and how it could
be fixed. To solve this problem, we modify the generator such that the precision
error is fixed when generating the system. Then the test framework is being
executed again to check whether both tests are succeeding. This resulted in the
same amount of tests that were failing, which means that we found a different
case now. In \autoref{tbl:experiment2_overview_second_run} an overview of the
used input values are shown\footnote{The decimals have been truncated for
readability, \autoref{lst:experiment2_log_second_run} shows the exact values}.
The log reported that one case still fails on the precondition check, while the
other case just reports values for which the result is \textit{false}, as shown
in \autoref{lst:experiment2_log_second_run}.
% Table
\FloatBarrier
\begin{table}[!ht]
\centering
\begin{tabular}{llll}
\hline
\textbf{Property name} & \textbf{x}                               & \textbf{y} & \textbf{z}                               \\ \hline
% Division1              & 1.504347826086956521739130434782609 USD  & -779       & -1171.886956521739130434782608695652 USD \\
Division1              & 1.5043478260... USD     & -779       & -1171.8869565217... USD \\
% Division2              & -3328.825454545454545454545454545455 USD & -129       & 25.80484848484848484848484848484848 USD  \\ \hline
Division2              & -3328.8254545454... USD & -129       & 25.8048484848... USD    \\ \hline
\end{tabular}
\caption{Failing tests overview, after fixing precision errors}
\label{tbl:experiment2_overview_second_run}
\end{table}
\FloatBarrier
% End table

% Listing
\FloatBarrier
\begin{sourcecode}[!ht]
\begin{lstlisting}[language=Log]
[info] MoneyConditionalsSpec:
[info] MoneyConditionals
[info] - should work with Additive4params (7 seconds, 24 milliseconds)
[info] - should work with AntisymmetryLET (3 seconds, 66 milliseconds)
[info] - should work with Symmetric (4 seconds, 361 milliseconds)
[info] - should work with Division2 *** FAILED *** (670 milliseconds)
[info]   java.lang.AssertionError: assertion failed: expected CommandSuccess(Division2(-3328.825454545454545454545454545455 USD,-129,25.80484848484848484848484848484848 USD)), found CommandFailed(NonEmptyList(PreConditionFailed(x == z*y)))
[info] - should work with Division1 *** FAILED *** (316 milliseconds)
[info]   java.lang.AssertionError: assertion failed: expected CurrentState(Result,Initialised(Data(None,Some(true)))), found CurrentState(Result,Initialised(Data(None,Some(false)))): With command: Division1(1.504347826086956521739130434782609 USD,-779,-1171.886956521739130434782608695652 USD)
// ...
\end{lstlisting}
\caption{Precondition failed error in \textit{Division1} and \textit{Division2}.}
\label{lst:experiment2_log_second_run}
\end{sourcecode}
\FloatBarrier
% End listing
\pinfo{Division2 triggers division problem}
The test concerning \textit{Division2} shows that the precondition check fails.
If we look at the input values, it can be seen that the \textit{Money} values
are a fractional number. As it contains many decimals and it rounds up at the
end. When operating with this rounded value, the resulting value is also
slightly different. As the generated system is implemented such that the
preconditions are being checked first, the \textit{PreConditionFailed} exception
is thrown. This leads to the issue of the division problem in which a number
cannot be equally divided. When defining the properties in
\autoref{cpt:properties}, we did not take this into account. Instead, we defined
that it should contain the exact value, which cannot be done in this case. This
should be defined in order to determine whether this is expected or incorrect.\\
\\
\pinfo{Division1 triggers difference in rounding problem}
% Division1 is defined as \code{x*y == z $\implies$ x == z/y}.
When looking at \textit{Division1}, we see another case as the input values
passed the precondition checks. This indicates that the values satisfy the
condition to trigger the if-clause of the property. However, the result of the
if-clause returns \textit{false}, showing us that the property does not hold
when using these input values. Thus a case has been found for which the
\textit{Division1} property doesn't hold. The investigation of the intermediate
calculation steps are shown in
\autoref{tbl:experiment2_division1_rounding_difference}. Note that the
\textit{Division1} property is defined as \code{x*y == z $\implies$ x == z/y}.

% Table
\FloatBarrier
\begin{table}[!ht]
\centering
\begin{tabular}{rll}
\hline
\textbf{Variable}  & \textbf{Value}                                    & \textbf{Type}                                        \\ \hline
X                  & 1.504347826086956521739130434782609 USD           & Money                                                \\
Y                  & -779                                              & Integer                                              \\
Z                  & -1171.886956521739130434782608695652 USD          & Money                                                \\ \hline
\textbf{Formula}   & \textbf{Scala result}                             & \textbf{Expected result}                             \\ \hline
x*y == z           & true                                              & false                                                \\
x == z/y           & false                                             & false                                                \\
                   &                                                   &                                                      \\
x*y                & -1171.886956521739130434782608695652 USD          & -1171.886956521739130434782608695652\textbf{411} USD \\
z/y                & 1.504347826086956521739130434782608 USD           & 1.504347826086956521739130434782608 USD              \\ \hline
\end{tabular}
\caption{Division1: Difference in rounding}
\label{tbl:experiment2_division1_rounding_difference}
\end{table}
\FloatBarrier
% End table
In the results, we can see that the expected values do not match the property
either. Although in \textit{Scala} the first expression is considered
\textit{true}. Since the expected results also return \textit{false} for the
intermediate calculations, the input values might not fully satisfy the
condition to trigger the if-clause. Which could be an implementation error in
our values generator. However, it's notable that in \textit{Scala} the condition
is considered to hold, which triggered this case. This indicates that there is a
rounding error happening in the system, which triggered this case.\\
\\
Unfortunately, we are unable to trace back how the input values used for this
tests were exactly determined. As these are build up by using randomly generated
values and then mutating these by a random operation (as described in
\autoref{sct:experiment2_method}). Nevertheless, the results show that there is
also an unexpected rounding going on when executing \textit{x*y} in
\textit{Scala}. As the expected value contains some additional decimals compared
to the result from \textit{Scala}.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Evaluation criteria
\section{Evaluation criteria}
\pinfo{Using 'fixed' version for evaluating}
To evaluate this experiment, we use the version of the generator in which the precision problems related to the \textit{Money} type are fixed. This is done because otherwise the precision errors would cause more tests to fail and the report would indicate a lower amount of coverage.\\
\\
\pinfo{Property coverage}
When looking at the coverage report concerning a specific implicative property,
it can be seen that the else-clause of the implication is not being triggered
anymore. This was also the intention of the modification done in this
experiment, as the if-clause is actually what we wanted to check in this
experiment. In
\autoref{fig:experiment2_eval_e2_highlighting_transitive-equality} the coverage
highlighting of \textit{TransitiveEquality} is shown.
% Figure
\FloatBarrier
\begin{figure}[!ht]
%\frame{
	\includegraphics[width=\linewidth]{figures/e2_coverage_implicative_property}
%}
\caption{Test coverage for \textit{TransitiveEquality} in second experiment}
\label{fig:experiment2_eval_e2_highlighting_transitive-equality}
\centering
\end{figure}
\FloatBarrier
% End figure

% Evaluation criteria
\pinfo{88,87\% - image}
The expectation was that the test framework could be improved, such that the
test coverage on the generated system would become higher. In the first
experiment we found that the implicative properties were not tested thoroughly. This is what has been improved in this experiment. The coverage report does not indicate a huge difference compared to the first experiment (\autoref{cpt:experiment1}). The total test coverage is 88,87\%, which is slightly higher than the results in the first experiment. The report is shown in \autoref{fig:experiment2_eval_e2}. Note that the properties have been categorized in this experiment, this is also visible in the coverage report (the two ``Logic'' files, one for each category).
% Figure
\FloatBarrier
\begin{figure}[!ht]
%\frame{
	\includegraphics[width=\linewidth]{figures/eval_e2}
%}
\caption{Test coverage report of the second experiment}
\label{fig:experiment2_eval_e2}
\centering
\end{figure}
\FloatBarrier
% End figure
\pinfo{Branch coverage, 50\%}
It is notable from the coverage report that the branch coverage is exactly 50\%. This amount is lower than what it was in the first experiment. However, in this experiment the implicative properties are being tested thoroughly, which was not the case in the first experiment. Considering that the
else-clause of the implicative properties is not being triggered, the test
coverage will never become 100\%. Which is not a problem in that sense, as we
don't intend to test the else clause, we are more interested in the result of
the if-clause of these implicative properties. This is also true for the branch coverage, which can be expected to be 50\%. Although the first experiment reported a higher amount for this, it was not testing the implicative properties good enough in the first experiment.\\
\\
\pinfo{\# of bugs, 2 more, incorrect definition}
The other criteria we use was the number of bugs that we have found. Using this
approach 2 more tests were failing compared to the first experiment in
\autoref{cpt:experiment1}. The division problem was not taken into account when
defining the properties, which resulted in these failing cases.\\
\\
When defining the properties, it was stated that the value should be the exact
value, while this is not possible with division. This results in a threat to the
definition of this property. Because of this, we consider the bugs that were
found as false positives. Although, because of this it might be the case that
the actual bugs it might be able to find, are being hidden because of this.
Resulting in false negatives: bugs that exist in the system, but are not found
by the test framework.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Conclusion
\section{Conclusion}
In this experiment, we generated the input values such that the condition of
the implicative properties are satisfied. This revealed 2 additional
failing cases that are triggering the division problem. The generated system
tries to hold the exact value, which triggers this situation. It is reasonable
that the system tries to hold the exact value. It is also not clear from the
property definitions what should happen in this case.\\
\\
When defining the properties in \autoref{cpt:properties}, we said that a value
should be precise. However, it is not possible to do this for all division
cases. Because of this, we consider the bugs that were found as false positives.
The definition of the property has to be updated in order to specify what should
happen in case such a division error occurs. Due to this error, it might be the
case that existing bugs in the generator are not being detected with this
approach.\\
\\
The property definitions should be updated such that it is known what should
happen in this case. In the next experiment we will focus on improving this.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Threats to validity
\section{Threats to validity}

% Incorrect value generation
\subsection*{Incorrect value generation}
We have implemented a custom value generator to generate values for each test
case. Furthermore, a random operation is being done on these values to make
these random again. There could be an error in the implementation that incorrect
values are being created, which are expected to be correct values. When this is
the case, the traceability of how the values were created is hard. This might
affect the results or make some errors hard to trace back.\\
\\
We have seen such an error when running this experiment multiple times. It sometimes happened that there were additional tests failing. Although these do not trigger in every run, leaving it as a threat to this approach. The error that caused these test to fail was that the preconditions are not met. This was only the case for some properties that are using ``smaller than'' ($<$) and ``greater than'' ($>$) in its definitions. The input values are set to all zeros, which results in the condition $0 > 0$, which results in \textit{false}. There is a high probability that this is caused by the random operation that is being done to mutate the input values. As mentioned earlier, the traceability for this is hard and since it does not happen regularly, fixing this issue is left as future work. Leaving it as a threat to our results.

% Detecting precision
%\subsection*{Detecting precision}
%\todo{Update definition}
%In this chapter, we triggered the division problem and found that this can
%cause problems in the generator. It's important to know what the expected result
%would be in this case. A possibility is to define this on the \textit{Rebel}
%language or to make such rounding and precision definitions part of the
%specification. Currently, it is unclear what should happen in this situation. We
%concluded that the generator doesn't take the rules on precision and rounding,
%that we have defined in \autoref{cpt:properties}, are not taken into account.
%And that it currently uses a lower precision. The precision we defined is
%perhaps not expected for \textit{Rebel} specifications, leaving it as a threat
%for this approach.
% Not testing for X decimals precision currently. We could generate values such that this isnt being triggered. However, maybe we want to do so. Or maybe its better to define the allocation or precision in Rebel, such that it is part of the specification. Currently this is unclear what should happen with division.

% Implicative properties
\subsection*{Implicative properties effectiveness}
\pinfo{Uneffective? Checking more though}
The use of implicative properties might not be as effective as using properties
that do not. If the properties could be rewritten such that random values could
be used to check the same thing, the implicative properties might be
unnecessary. On the other hand, more functionalities from the generator are
being used, and thus being tested, by this approach. Which wouldn't be the case
when the implicative properties are being removed. If-statements and
preconditions were not being used in the first experiment.

% Not extra coverage maybe? If we evaluate that different than earlier.
