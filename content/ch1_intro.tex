% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Chapter: Introduction
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\chapter{Introduction}
\label{chp:intro}
\pinfo{Short about Rebel}
Large systems often suffer from domain knowledge that is implicit, incomplete, out of date or ambiguous definitions. This is what \textit{Rebel} aims to solve \cite{stoel2016solving}. The toolchain of \textit{Rebel} can be used to check, simulate and visualize the specifications, allowing to reason about the final product \cite{stoelcase}. Checking is done based on bounded model checking by using the Z3 solver.\\
\\
\pinfo{Aim of project}
Generators are being used to generates a system from the Rebel specifications. The generated system provides an API interface in order to work with the specified product and handles the database connectivity. However, the implementation of the generated program is not checked against the specifications, meaning that the generated program is perhaps not doing what it is supposed to do according to its specifications. The aim of this project is to improve this, by automatically testing the generated program against Rebel specifications.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Initial study
% // Don't think it's needed? Rebel papers?
% // It's in background too, about Rebel and what Rebel does.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Problem statement
\section{Problem statement}
\pinfo{Translation from generator and why automatically}
From the \textit{Rebel} specifications, a system can be generated by the generator. However, neither the generator, nor the generated program is being tested against the specification. Thus it could be that the generated system doesn't work according to what is specified in the \textit{Rebel} specification. Although the generator should translate everything correctly, we cannot assume that it actually does translate it correctly for each case and that the implementation works as expected.\\
\\
Currently, there are no tests for the generator or the generated system, during the development of the generator the results are being checked manually. Testing is a major cost factor in Software Development, with test automation being proposed as one of the solutions to reduce these costs \cite{ramler2006economic}. We aim for an approach such that much of the testing is automated to reduce the time (and costs) needed for testing certain components of the generated system.\\
\\
The main research question is as follows:
\begin{quote}
  \rqMain
\end{quote}
We investigate the following solution: generating tests based on a \textit{Rebel} specification and then run these tests against the generated system.\\
\\
% - Property based testing is a possible path, which we will do.
\pinfo{Possibility: PBT + short explanation}
Property-based testing is an approach to validate whether an implementation satisfies its specification~\cite{fink1997property}. It describes what the program should or should not do. As~\cite{fink1997property} describes: ``Property-based testing validates that the final product is free of specific flaws.''. With property-based testing, a property is being defined which should hold on the system. Next, the property is being tested for a certain amount of tries, using different input values to check whether the property holds. In case the property doesn't hold, it will result in a failure, reporting that there is a case in which the property doesn't hold. Indicating that a bug in the system has been triggered.\\
\\
\pinfo{Doing it too, worked already on x, x and x}
Property-based testing has already shown a success in earlier studies~\cite{fink1997property,claessen2011quickcheck,arts2006testing}, detecting errors in a system that were not known before. In this thesis we will use property-based testing to check the generator, using the generated system to check whether the properties hold.\\
\\
% - Hypothesis? We will find bugs in the generated system using this approach. Although errors could have been found (and fixed) already by manual testing/checking and code reviews,  we expect to detect yet unknown bugs in the generated system.
\pinfo{Hypothesis, we will detect}
We hypothesize that there are yet unknown bugs in the generator, resulting in that the generated system does not work as expected. By using property-based testing we expect to detect bugs in the generator.\\
\\
To answer the main research question, we will first answer the following research questions:
\begin{description}
\item[~~~~RQ 1:] \rqOne
\item[~~~~RQ 2:] \rqTwo
\item[~~~~RQ 3:] \rqThree
\end{description}

% - Shortly describe how this is done, and how it would find bugs, test mechanics explain it in more detail, image in test mechanics.
\pinfo{In short: how. Details are in CH3}
\todo{Is this needed here? Maybe remove or place somewhere else?}
The generator takes a \textit{Rebel} specification as input. Which contains the properties that are expected to hold in the generated system. Next, \textit{Scala} tests are being generated based on the properties, using the existing generator to translate the expressions used in the properties. These tests will be run against the generated system, to check whether each property holds. In case a test of a failing test, a bug has been found. There are multiple generators available within ING. Throughout this thesis, we will use the most mature generator, which is the Scala/Akka generator. This generator is often used for other experiments too.\\
\\
\pinfo{Assumptions}
In order to run the test suit, we assume that the generated system can be compiled and that it can be run. Furthermore, the specification which was used to generate the system should be syntactically and semantically correct. Which means that the \textit{Rebel} type checker should not report any error about the specification.\\
\\
\pinfo{Not detecting everything, but checking properties}
The test framework generates a test suite that can be run against the generated system. In case the test suite finishes without errors, it means that it did not found any bugs and that the generator satisfies the properties that were tested. This doesn't mean that there are no bugs in the generated system, instead, it means that our test suite was not able to find errors in the properties that it checks for. The generated system will probably still contain bugs which are not detected by using the test framework. In this case, improving the test framework might extend the number of bugs that it can find.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Research method
\section{Research method}
% In short, describe steps:
% // (Steps also interesting for mechanics chapter)
% 1 Defining properties, define what is unknown yet and what it actually means.
% 2 Describe how tests can be generated out of this. From these properties to Scala tests. (Properties > Rebel events > Scala tests per event, with random values)
% 3 Describe how we measure our experiments
% 4 Evaluate the results. Can we reason about it? What have we covered and also what have we not covered?
\pinfo{First defining properties, then small example}
We will start off with defining the properties that are expected to hold on the generator. Then we describe how these properties can be tested on the generator, using one property to demonstrate the working of the test framework. We can then run the tests suit against the generated system and check if this method actually works to detect bugs.\\
\\
\pinfo{Generate, run, evaluate results and improve again}
Next, we generate tests for each of these properties and run these against the generated system. After running the test suit, the result is being evaluated. When one or more tests are failing, a bug is found. However, we need to investigate the failing case such that we discover what the actual bug is. After evaluating we improve our test generation and continue to evaluate the results again.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Contribution
\section{Contribution}
% - Definitions (properties)
% Before, in conclusion:
% We provided a definition of some types in \textit{Rebel} and described properties that should hold when using operations with these types. Since there were no clear definitions available yet of what these types exactly do, a contribution has been made to the \textit{Rebel} project, such that there are now some definitions and properties available. This also helps when reasoning about the implementation, as there is now a definition of what is expected. Although these definitions might change over time, it allows to reason back to an earlier starting point.\\
We provide definitions of the properties that are expected to hold when using the \textit{Rebel} language and its toolchain. Allowing to reason about the generator and whether the implementation satisfies these properties. There are no property definitions of \textit{Rebel} yet, so we provide a starting point for this with the focus on the important properties.\\
\\
% - A way to automatically test the generator and generated system using Rebel, defining properties as events
% Before, in conclusion:
% We have shown a way to automatically test the generator using the \textit{Rebel} language and its toolchain, with the addition of our test framework. This is done by defining the properties that are expected to hold in the first case. Which can be translated to a \textit{Rebel} specification such that a system can be generated from it. Additionally using the generator when generating the test cases, so that the translation of expressions in the generator will be checked too. The different kind of properties should be able to detect unexpected behaviour.\\
The defined properties are being used to check the generator. We come with a solution that uses the input that is required for the generator and the output of the generator to check the generator. This process should be as automatic as possible, such that it doesn't require much time to make use of it. Our test framework will combine the required steps to detect bugs as automatically as possible.\\
\\
% - Found number of bugs in generated system
% Before, in conclusion:
%A number of bugs have been found in the generated system, precision errors, overflow/underflow errors and even a compile error. These bugs are now known issues in this project, while these were unknown before. This shows us that this approach already worked in such a way that it is able to detect bugs in the generated system. It can be extended to detect even more bugs. With some modifications, it is also possible to use this approach with other generated systems, such that it can detect inequalities among these.\\
The generator that is being used is expected to contain bugs. Therefore, we expect to detect, yet unknown, bugs in the generator. These bugs are then known bugs and can be fixed. The bugs found will then also indicate what kind of bugs we can detect in a generator by using this approach.

% - Squants issues
% Before, in conclusion:
%In this approach, we identified two bugs, which were due to an open-source library used in the generated system, called \textit{Squants}. We reported these two issues, such that these can be fixed in a later version of the library.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Related work
\section{Related work}
% // Doesn't this have to be at the end?
% - Testing of (extended) finite state machines

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Subsection: Random testing
\subsection{Random testing}
\pinfo{Describe random testing (FDRT)}
Random testing is a technique in which random values are being used as input for the test cases. \textit{QuickCheck} \cite{claessen2011quickcheck} and \textit{Randoop} \cite{pacheco2007randoop} are examples of random testing techniques. These differ in how they automatically test systems and what actually is being tested. QuickCheck is based on property-based testing, which we also apply in this project. \textit{Randoop} on the other hand is based on feedback-directed random testing.\\
\\
\pinfo{Describe FDRT}
With feedback-directed random testing, random tests are generated which will immediately be run. The result of earlier test attempts can affect the next test that is being generated, which can be seen as feedback for the next generated test. This allows each test case to 'learn' from earlier attempts and to create unique tests.\\
\\
\pinfo{About Randoop, shortly how it works}
\textit{Randoop} is build for Java projects and checks some built-in specifications of Java that can't be checked by the compiler. The test cases are simple unit tests, consisting of a unique sequence of methods due to the feedback of earlier attempts. The method sequences are unique because it also checks whether the same case has already been checked. Since there can be unlimited sequences of methods to test, the test suite will be terminated after a defined timeout. Next, the result is determined and failing cases are being reported, although when using this approach it cannot determine whether the whole system is correct according to the Java specifications. Instead, it just wasn't able to find a case for which it fails. This is a useful approach to generate unique tests, but its goal is to check systems built in Java and thus is not compatible with the semantics of Rebel. It works with calling the Java objects and using methods on those, while the generated system consists of mainly states and events. When using an approach like \textit{Randoop} with random input values, it cannot be known whether the result of a specific transition was expected to succeed or fail.\\
% Can add DART here, if needed
\\
\pinfo{Our case, why existing approaches can't be used}
%  Background contains info about quickcheck and randoop. Repeat limitations and describe how we do it
Approaches like \textit{QuickCheck} \cite{claessen2011quickcheck} and \textit{Randoop} \cite{pacheco2007randoop} enforce the system under test to be written in a specific language (\textit{Haskell} for \textit{QuickCheck}, \textit{Java} for \textit{Randoop}). For \textit{QuickCheck} there are alternative solutions for other languages. In our case, we need to use \textit{Rebel} when generating test cases, such that we test the generator when generating the tests. Another reason why we can't use a method like \textit{Randoop} is that \textit{Randoop} strictly checks for \textit{Java} properties, which are not in line with the \textit{Rebel} language.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Outline
\section{Outline}
\todo[inline]{Write later, when outline is more final.}
% 1. Describing background
% 2. Defining properties
% 3. Test mechanics
% 4. Experiment 1
% 5. Experiment 2
% 6. Conclusion
% 7. Threats of validity
% 8. Future work?
