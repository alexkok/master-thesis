% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Chapter: Introduction
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
\chapter{Introduction}
\label{chp:intro}
\pinfo{Short about Rebel}
Large systems often suffer from domain knowledge that is implicit, incomplete, out of date or ambiguous definitions. This is what Rebel aims to solve \cite{stoel2016solving}. The toolchain of Rebel allows to check, simulate and visualize the specifications, allowing to reason about the final product \cite{stoelcase}. Checking is done based on bounded model checking by using the Z3 solver.\\
\\
\pinfo{Aim of project}
Generators are being used to generates a system from the Rebel specifications. The generated system provides an API interface in order to work with the specified product and handles the database connectivity. However, the implementation of the generated program is not checked against the specifications, meaning that the generated program is perhaps not doing what it is supposed to do according to its specifications. The aim of this project is to improve this, by automatically testing the generated program against Rebel specifications.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Initial study
% // Don't think it's needed? Rebel papers? 
% // It's in background too, about Rebel and what Rebel does.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Problem statement
\section{Problem statement}
\pinfo{Translation from generator and why automatically}
From the Rebel specifications, a system can be generated by the generator. However, neither the generator nor the generated program is being tested against the specification. Thus it could be that the generated system doesn't work according to what is specified in the Rebel specification. Although the generator should translate everything correctly, we cannot assume that it actually does translate it correctly for each case and that the implementation works as expected. Currently, there are no tests for the generator or the generated system, during the development of the generator the results are being checked manually. Testing is a major cost factor in Software Development, with test automation being proposed as one of the solutions to reduce these costs \cite{ramler2006economic}. We aim for an approach such that much of the testing is automated to reduce the time (and costs) needed for testing certain components of the generated system.\\
\\
The main research question is as follows:
\begin{quote}
  \rqMain
\end{quote}
We investigate the following solution: generating tests based on a Rebel specification, and then run these tests against the generated system.\\
\\
% - Property based testing is a possible path, which we will do.
\pinfo{Possibility: PBT + short explanation}
Property-based testing is an approach to validate whether an implementation satisfies its specification~\cite{fink1997property}. It describes what the program should or should not do. As~\cite{fink1997property} describes: "Property-based testing validates that the final product is free of specific flaws.". With property-based testing a property is being defined which should hold on the system. Next, the property is being tested for a certain amount of tries, using different input values to check whether the property holds. In case the property doesn't hold, it will result in a failure, reporting that there is a case in which the property doesn't hold. Indicating that a bug in the system has been triggered.\\
\\
\pinfo{Doing it too, worked already on x, x and x}
Property based testing has already shown a success in earlier studies~\cite{fink1997property,claessen2011quickcheck,arts2006testing}, detecting errors in a system that were not known before. In this project we will use property-based testing to check the generator, using the generated system to check whether the properties hold.\\
\\
% - Shortly describe how this is done, and how it would find bugs, test mechanics explain it in more detail, image in test mechanics.
\pinfo{In short: how. Details are in CH3}
The generator takes a \textit{Rebel} specification as input. Which contains the properties which are expected to hold in the generated system. Next, Scala tests are being generated based on the properties, using the existing generator to translate the expressions used in the properties. These tests will be run against the generated system, to check whether each property holds. In case a test of a failing test, a bug has been found.\\
\\
% - Hypothesis? We will find bugs in the generated system using this approach. Although errors could have been found (and fixed) already by manual testing/checking and code reviews,  we expect to detect yet unknown bugs in the generated system.
\pinfo{Hypothesis, we will detect}
There are multiple generators available within ING. Throughout this project we will use the most mature generator, which is the Scala/Akka generator. This generator is often used for other experiments too. We hypothesize that there are yet unknown bugs in the generator, resulting in that the generated system does not work as expected. By using property-based testing we expect to detect bugs in the generator.\\
\\
\pinfo{Assumptions}
In order to be able to test the generated system, we assume that the generated system can be compiled and that it can be run. Furthermore, the specification on which the generated system is based on should be syntactically and semantically correct. Which means that the \textit{Rebel} type checker should not report any error about the specification.\\
\\
To answer the main research question, we will first answer the following research questions:
\begin{description}
\item[~~~~RQ 1:] \rqOne
\item[~~~~RQ 2:] \rqTwo
\item[~~~~RQ 3:] \rqThree
\end{description}
\pinfo{Not detecting everything, but checking properties}
The test framework generates a test suit that can be run against the generated system. In case the test suite finishes without errors, it means that it did not found any bugs and that the generator satisfies the properties that were tested. This doesn't mean that there are no bugs in the generated system, instead, it means that our test suite was not able to find errors in the properties that it checks for. The generated system will probably still contain bugs which are not detected by using the test framework. In this case, improving the test framework might extend the amount of bugs that it can find.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Research method
\section{Research method}
% In short, describe steps:
% // (Steps also interesting for mechanics chapter)
% 1 Defining properties, define what is unknown yet and what it actually means.
% 2 Describe how tests can be generated out of this. From these properties to Scala tests. (Properties > Rebel events > Scala tests per event, with random values)
% 3 Describe how we measure our experiments
% 4 Evaluate the results. Can we reason about it? What have we covered and also what have we not covered?
\pinfo{First defining properties and then small example}
We will start off with defining the properties that are expected to hold on the generator. Then we describe how these properties can be tested on the generator, using one property to demonstrate the working of the test framework. We can then run the tests suit against the generated system and check if this method actually works to detect bugs.\\
\\
\pinfo{Generate, run, evaluate results and improve again}
Next, we generate tests for each of these properties automatically and run these against the generated system. After running the test suit, the result is being evaluated. When one or more tests are failing, a bug is found. However, we need to investigate the failing case such that we discover what the actual bug is. After evaluating we improve our test generation and continue to evaluate the results again.

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Related work
\section{Related work}
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Random testing
\subsection{Random testing}
\pinfo{Describe random testing (FDRT)}
Random testing is a technique in which random values are being used as input for the test cases. \textit{QuickCheck} \cite{claessen2011quickcheck} and \textit{Randoop} \cite{pacheco2007randoop} are examples of random testing techniques. These differ in how they automatically test systems and what actually is being tested. QuickCheck is based on property-based testing, which we also apply in this project. \textit{Randoop} on the other hand is based on feedback-directed random testing.\\
\\
\pinfo{Describe FDRT}
With feedback-directed random testing, random tests are generated which will immediately be run. The result of earlier test attempts can affect the next test that is being generated, which can be seen as feedback for the next generated test. This allows each test case to 'learn' from earlier attempts and to create unique tests.\\
\\
\pinfo{About Randoop, shortly how it works}
\textit{Randoop} is build for Java projects and checks some built-in specifications of Java that can't be checked by the compiler. The test cases are simple unit tests, consisting of a unique sequence of methods due to the feedback of earlier attempts. The method sequences are unique because it also checks whether the same case has already been checked. Since there can be unlimited sequences of methods to test, the test suite will be terminated after a defined timeout. Next, the result is determined and failing cases are being reported, although when using this approach it cannot determine whether the whole system is correct according to the Java specifications. Instead, it just wasn't able to find a case for which it fails. This is a useful approach to generate unique tests, but its goal is to check systems built in Java and thus is not compatible with the semantics of Rebel. It works with calling the Java objects and using methods on those, while the generated system consists of mainly states and events. When using an approach like \textit{Randoop} with random input values, it cannot be known whether the result of a specific transition was expected to succeed or fail.
% Can add DART here, if needed


% // Doesn't this have to be at the end?
% Testing of (extended) finite state machines

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% Section: Outline
\section{Outline}
\todo[inline]{Write later, when outline is more final.}
% 1. Describing background
% 2. Defining properties
% 3. Test mechanics
% 4. Experiment 1
% 5. Experiment 2
% 6. Conclusion
% 7. Threats of validity
% 8. Future work?



























